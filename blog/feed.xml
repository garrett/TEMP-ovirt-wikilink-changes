<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>oVirt</title>
  <subtitle/>
  <id>http://ovirt.org/blog/</id>
  <link href="http://ovirt.org/blog/"/>
  <link href="http://ovirt.org/blog/feed.xml" rel="self"/>
  <updated>2016-04-25T16:00:00+02:00</updated>
  <author>
    <name/>
  </author>
  <entry>
    <title>Advanced users authentication, using Kerberos, CAS SSO and Active Directory</title>
    <link rel="alternate" href="http://ovirt.org/blog/2016/04/sso/"/>
    <id>http://ovirt.org/blog/2016/04/sso/</id>
    <published>2016-04-25T16:00:00+02:00</published>
    <updated>2016-05-24T17:02:06+02:00</updated>
    <author>
      <name>fbacchella</name>
    </author>
    <content type="html">&lt;p&gt;I have a environment where hard coded password are avoided. We prefer to use Kerberos. We also provided a SSO for Web UI using &lt;a href="http://jasig.github.io/cas/4.2.x/index.html"&gt;CAS&lt;/a&gt;. We use ActiveDirectory for users backend.&lt;/p&gt;

&lt;p&gt;So I wanted a oVirt installation that will use kerberos for API authentication. For the web UI, Kerberos is not always the best solution, so I wanted to integrated it in our CAS.&lt;/p&gt;

&lt;p&gt;The Apache part was easy to setup. It needs an external module, auth_cas_module, that can be found at &lt;a href="https://wiki.jasig.org/display/CASC/mod_auth_cas"&gt;Apache's CAS module&lt;/a&gt;. It builds without special tweaks with&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;./configure
make
make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I will show only subpart of the whole Apache setup and only authentication related part&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;# The CAS modules
LoadModule authz_user_module      /usr/lib64/httpd/modules/mod_authz_user.so
# Needed because auth_cas_module forget to link openssl
LoadModule ssl_module            /usr/lib64/httpd/modules/mod_ssl.so
LoadModule auth_cas_module       /usr/lib64/httpd/modules/mod_auth_cas.so

# For the kerberos authentication on the API
LoadModule auth_gssapi_module    /usr/lib64/httpd/modules/mod_auth_gssapi.so
LoadModule session_module        /usr/lib64/httpd/modules/mod_session.so
LoadModule session_cookie_module /usr/lib64/httpd/modules/mod_session_cookie.so

CASLoginURL https://sso/cas/login
CASValidateSAML On
CASValidateURL https://sso/cas/samlValidate

&amp;lt;VirtualHost *:443&amp;gt;

    RequestHeader unset X-Remote-User early
    
    &amp;lt;LocationMatch ^/api($|/)&amp;gt;
        RequestHeader set X-Remote-User %{REMOTE_USER}s

        RewriteEngine on
        RewriteCond %{LA-U:REMOTE_USER} ^(.*@DOMAIN)$
        RewriteRule ^(.*)$ - [L,P,E=REMOTE_USER:%1]

        AuthType GSSAPI
        AuthName "GSSAPI Single Sign On Login"
        GssapiCredStore keytab:.../httpd.keytab
        Require valid-user
        GssapiUseSessions On
        Session On
        SessionCookieName ovirt_gssapi_session path=/private;httponly;secure;
    &amp;lt;/LocationMatch&amp;gt;
    
    &amp;lt;LocationMatch ^/(ovirt-engine($|/)|RHEVManagerWeb/|OvirtEngineWeb/|ca.crt$|engine.ssh.key.txt$|rhevm.ssh.key.txt$)&amp;gt;
        AuthType CAS
        Require valid-user
        CASAuthNHeader X-Remote-User
    &amp;lt;/LocationMatch&amp;gt;
&amp;lt;/VirtualHost&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The file httpd.keytab contains the kerberos ticket for the service HTTP. In my setup, the realm using for Linux machine is different 
than the active directory's domain, and a trust was established between them. So the keytab is created using MIT kerberos.&lt;/p&gt;

&lt;p&gt;It was generated using the following &lt;code&gt;kadmin&lt;/code&gt; commands:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;addprinc -randkey HTTP/VHOST@REALM
addprinc -randkey HTTP/FQDN@REALM
ktadd -k .../http.keytab -e aes128-cts-hmac-sha1-96:normal -e aes256-cts-hmac-sha1-96:normal HTTP/VHOST@REALM
ktadd -k .../http.keytab -e aes128-cts-hmac-sha1-96:normal -e aes256-cts-hmac-sha1-96:normal HTTP/FQDN@REALM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kerberos can be surprising when resolving principal and http client uses different method. Some requests an ticket using directly the Host header. Some other choose the reverse of the IP used for the connection.
So if Apache is configured using a virtual host, both principal for the virtual host and the FQDN pointed by the reverse of the IP should be created and added to the keytab.&lt;/p&gt;

&lt;p&gt;The authn file &lt;code&gt;/etc/ovirt-engine/extensions.d/apachesso-authn.properties&lt;/code&gt; is :&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;ovirt.engine.extension.name = apachesso-authn
ovirt.engine.extension.bindings.method = jbossmodule
ovirt.engine.extension.binding.jbossmodule.module = org.ovirt.engine-extensions.aaa.misc
ovirt.engine.extension.binding.jbossmodule.class = org.ovirt.engineextensions.aaa.misc.http.AuthnExtension
ovirt.engine.extension.provides = org.ovirt.engine.api.extensions.aaa.Authn
ovirt.engine.aaa.authn.profile.name = apachesso
ovirt.engine.aaa.authn.authz.plugin = DOMAIN-authz
config.artifact.name = HEADER
config.artifact.arg = X-Remote-User
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the authz file &lt;code&gt;/etc/ovirt-engine/extensions.d/DOMAIN-authz.properties&lt;/code&gt; is:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;ovirt.engine.extension.name = DOMAIN-authz
ovirt.engine.extension.bindings.method = jbossmodule
ovirt.engine.extension.binding.jbossmodule.module = org.ovirt.engine-extensions.aaa.ldap
ovirt.engine.extension.binding.jbossmodule.class = org.ovirt.engineextensions.aaa.ldap.AuthzExtension
ovirt.engine.extension.provides = org.ovirt.engine.api.extensions.aaa.Authz
config.profile.file.1 = ..../aaa/DOMAIN.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I had some difficulties with AD backend. A straightforward solution would have been :&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;include = &amp;lt;ad.properties&amp;gt;

vars.domain = DOMAIN
vars.user = BINDDN
vars.password = BINDPWD
vars.forest = domain.com

pool.default.auth.simple.bindDN = ${global:vars.user}
pool.default.auth.simple.password = ${global:vars.password}
pool.default.serverset.type = srvrecord
pool.default.serverset.srvrecord.domain = ${global:vars.domain}

pool.default.ssl.startTLS = true
pool.default.ssl.truststore.file = .../domain.jks
pool.default.ssl.truststore.password = 
# Only TLSv1.2 is secure nowadays
pool.default.ssl.startTLSProtocol = TLSv1.2

# long time out should be avoided
pool.default.connection-options.connectTimeoutMillis = 500
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But if fails. We have a special setup with about 100 domain controlers and only two of them can be reached from the ovirt engine. So my first try was so defined them directly in the configuration file:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;pool.default.serverset.type = failover
pool.default.serverset.failover.1.server = dcX.domain.com
pool.default.serverset.failover.2.server = dcY.domain.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But that fails. oVirt-engine was still using a lot of unreachable domain controler. After some digging I found that other part of the ldap extension use a different serverset, I don’t know why it don’t reuse the default pool. It’s called &lt;code&gt;pool.default.dc-resolve&lt;/code&gt; (it should be called &lt;code&gt;pool.dc-resolve&lt;/code&gt;, as it’s not the default but a custom one), so I added in my configuration:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;pool.default.dc-resolve.default.serverset.type = failover
pool.default.dc-resolve.serverset.failover.1.server = dcX.domain.com
pool.default.dc-resolve.serverset.failover.2.server = dcY.domain.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I worked well, but there is a better solution as Ondra Machacek point it to me. In Active Directory, there is something called a “site”, with a subset of all the domain controler in it. It can be found under &lt;code&gt;CN=Sites,CN=Configuration,DC=DOMAIN,...&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To list them:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;ldapsearch -H ldap://somedc -b CN=Sites,CN=Configuration,DC=DOMAIN -s one -o ldif-wrap=no cn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The information to write down is the &lt;code&gt;cn&lt;/code&gt; returned&lt;/p&gt;

&lt;p&gt;You get a list of all sites, just pick the right one, remove all the serverset configuration and add :&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;pool.default.serverset.srvrecord.domain-conversion.type = regex
pool.default.serverset.srvrecord.domain-conversion.regex.pattern = ^(?&amp;lt;domain&amp;gt;.*)$
pool.default.serverset.srvrecord.domain-conversion.regex.replacement = GOOD_SITE._sites.${domain}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The entry &lt;code&gt;_sites.${domain}&lt;/code&gt; don’t exist in the DNS, so to check that your regex is good, try instead:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;dig +short _ldap._tcp.GOOD_SITE._sites.${domain} srv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It should return only reachable domain controlers.&lt;/p&gt;

&lt;p&gt;So the final &lt;code&gt;/etc/ovirt-engine/aaa/DOMAIN.properties&lt;/code&gt; was :&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;include = &amp;lt;ad.properties&amp;gt;

vars.domain = DOMAIN
vars.user = BINDDN
vars.password = BINDPWD
vars.forest = domain.com

pool.default.auth.simple.bindDN = ${global:vars.user}
pool.default.auth.simple.password = ${global:vars.password}
pool.default.serverset.type = srvrecord
pool.default.serverset.srvrecord.domain = ${global:vars.domain}

pool.default.ssl.startTLS = true
pool.default.ssl.truststore.file = .../domain.jks
pool.default.ssl.truststore.password = 
pool.default.ssl.startTLSProtocol = TLSv1.2

pool.default.connection-options.connectTimeoutMillis = 500

pool.default.serverset.srvrecord.domain-conversion.type = regex
pool.default.serverset.srvrecord.domain-conversion.regex.pattern = ^(?&amp;lt;domain&amp;gt;.*)$
pool.default.serverset.srvrecord.domain-conversion.regex.replacement = GOOD_SITE._sites.${domain}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this setup, my &lt;a href="https://github.com/fbacchella/ovirtcmd"&gt;python client&lt;/a&gt; can connect to ovirt-engine using kerberos ticket, web users are authenticated using CAS. And there is no need to duplicate user base.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Up and Running with oVirt 3.6 and Gluster Storage</title>
    <link rel="alternate" href="http://ovirt.org/blog/2016/03/up-and-running-with-ovirt-3-6/"/>
    <id>http://ovirt.org/blog/2016/03/up-and-running-with-ovirt-3-6/</id>
    <published>2016-03-18T09:00:00+01:00</published>
    <updated>2016-05-18T16:20:26+02:00</updated>
    <author>
      <name>Jason Brooks</name>
    </author>
    <content type="html">&lt;p&gt;In November, version 3.6 of oVirt, the open source virtualization management system, &lt;a href="http://lists.ovirt.org/pipermail/announce/2015-November/000205.html"&gt;hit FTP mirrors&lt;/a&gt; featuring a whole slate of &lt;a href="/develop/release-management/releases/3.6/"&gt;fixes and enhancements&lt;/a&gt;, including support for storing oVirt's self hosted management engine on a &lt;a href="/develop/release-management/features/engine/self-hosted-engine-gluster-support/"&gt;Gluster volume&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This expanded Gluster support, along with the new &lt;a href="https://gluster.readthedocs.org/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/"&gt;"arbiter volume"&lt;/a&gt; feature added in &lt;a href="http://blog.gluster.org/2015/05/glusterfs-3-7-0-has-been-released-introducing-many-new-features-and-improvements/"&gt;Gluster 3.7&lt;/a&gt;, has allowed me to simplify (somewhat) the converged oVirt+Gluster installation &lt;a href="http://community.redhat.com/blog/2014/11/up-and-running-with-ovirt-3-5-part-two/"&gt;that's powered&lt;/a&gt; my &lt;a href="http://community.redhat.com/blog/2014/05/ovirt-3-4-glusterized/"&gt;test lab&lt;/a&gt; for the &lt;a href="http://community.redhat.com/blog/2013/09/ovirt-3-3-glusterized/"&gt;past few years&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read on to learn about my favored way of running oVirt, using a trio of servers to provide for the system's virtualization and storage needs, in a configuration that allows you to take one of the three hosts down at a time without disrupting your running VMs.&lt;/p&gt;

&lt;p&gt;IMPORTANT NOTE:&lt;/p&gt;

&lt;p&gt;I want to stress that this converged virtualization and storage scenario is a bleeding-edge configuration. Many of the ways you might use oVirt and Gluster are available in commercially-supported configurations using RHEV and RHS, but at this time, this oVirt+Gluster mashup isn't one of them. What's more, this configuration is not "supported" by the oVirt project proper, a state that should change somewhat once this &lt;a href="http://www.ovirt.org/develop/release-management/features/engine/self-hosted-engine-hyper-converged-gluster-support"&gt;Self Hosted Engine Hyper Converged Gluster Support&lt;/a&gt; feature lands in oVirt.&lt;/p&gt;

&lt;p&gt;If you're looking instead for a simpler, single-machine option for trying out oVirt, here are a pair of options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href="http://www.ovirt.org/OVirt_Live"&gt;oVirt Live ISO&lt;/a&gt;: A LiveCD image that you can burn onto a blank CD or copy onto a USB stick to boot from and run oVirt. This is probably the fastest way to get up and running, but once you're up, this is definitely a low-performance option, and not suitable for extended use or expansion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href="http://www.ovirt.org/develop/release-management/features/integration/allinone/"&gt;oVirt All in One plugin&lt;/a&gt;: Run the oVirt management server and virtualization host components on a single machine with local storage. The setup steps for AIO haven't changed much since I wrote about it &lt;a href="http://community.redhat.com/blog/2013/09/up-and-running-with-ovirt-3-3/"&gt;two years ago&lt;/a&gt;. This approach isn't too bad if you have limited hardware and don't mind bringing the whole thing down for maintenance, but oVirt really shines brightest with a cluster of virtualization hosts and some sort of shared storage.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id="ovirt-glusterized"&gt;oVirt, Glusterized&lt;/h2&gt;

&lt;h3 id="prerequisites"&gt;Prerequisites&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; You’ll need three machines with plenty of RAM and processors with &lt;a href="http://en.wikipedia.org/wiki/X86_virtualization#Hardware-assisted_virtualization"&gt;hardware virtualization extensions&lt;/a&gt;. Physical machines are best, but you can test oVirt using &lt;a href="http://community.redhat.com/blog/2013/08/testing-ovirt-3-3-with-nested-kvm/"&gt;nested KVM&lt;/a&gt; as well. I've written this howto using VMs running on my "real" oVirt+Gluster install.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Software:&lt;/strong&gt; For this howto, I'm using CentOS 7 for both the host and the Engine VM. oVirt does support other OS options. For more info see the project's &lt;a href="http://www.ovirt.org/download/"&gt;download page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Network:&lt;/strong&gt; Your test machine’s host name must resolve properly, either through your network’s DNS, or through the &lt;code&gt;/etc/hosts&lt;/code&gt; file on your virt host(s), on the VM that will host the oVirt engine, and on any clients from which you plan on administering oVirt. It's not strictly necessary, but it's a good idea to It's a good idea to set aside a separate storage network for Gluster traffic and for VM migration. In my lab, I use a separate 10G nic on each of the hosts for my storage network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; The hosted engine feature requires NFS, iSCSI, FibreChannel or Gluster storage to house the VM that will host the engine. For this walkthrough, I'm using a Gluster arbiter volume, which involves creating a replica 3 Gluster volume with two standard data bricks and a third arbiter brick that stores only file names and metadata, thereby providing an oVirt hosted engine setup with the data consistency it requires, while cutting down significantly on duplicated data and network traffic. If you have access to good external storage to use with your oVirt exploration, you can skip the Gluster setup bits, and I'll point out where you can substitute your external storage resource.&lt;/p&gt;

&lt;h3 id="installing-ovirt-with-hosted-engine"&gt;Installing oVirt with hosted engine&lt;/h3&gt;

&lt;p&gt;I'm starting out with three test machines with 16 GB of RAM and 4 processor cores, running a minimal installation of CentOS 7 with all updates applied.  I actually do the testing for this howto in VMs hosted on my "real" oVirt setup, but that "real" setup closely resembles what I describe below.&lt;/p&gt;

&lt;p&gt;I've identified a quartet of static IP address on my network to use for this test (three for my virt hosts, and one for the hosted engine). In addition, I'm using three other static IPs, on a different network, for Gluster storage. I've set up the DNS server in my lab to make these IPs resolve properly, but you can also edit the /etc/hosts files on your test machines for this purpose.&lt;/p&gt;

&lt;p&gt;Next, we need to configure the oVirt software repository on each of our three hosts.&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# yum install -y http://resources.ovirt.org/pub/yum-repo/ovirt-release36.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, install the hosted engine packages, along with &lt;a href="http://www.gnu.org/software/screen/"&gt;screen&lt;/a&gt;, which can come in handy during the deployment process:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# yum install -y ovirt-hosted-engine-setup screen glusterfs-server vdsm-gluster system-storage-manager
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="gluster-preparations-run-on-each-host"&gt;Gluster preparations (run on each host)&lt;/h3&gt;

&lt;p&gt;We need a partition to store our Gluster bricks. For simplicity, I'm using a single XFS partition, and my Gluster bricks will be directories within this partition. I use system-storage-manager to manage my storage.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you're skipping the local Gluster storage, and using some different external storage, you can skip this step.&lt;/em&gt;&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# ssm add -p gluster $YOUR_DEVICE_NAME
[each-host]# ssm create -p gluster --fstype xfs -n bricks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, create a mountpoint and modify your &lt;code&gt;/etc/fstab&lt;/code&gt; to ensure that your LVM volume gets mounted at boot time:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# mkdir /gluster
[each-host]# echo "/dev/mapper/gluster-bricks /gluster xfs defaults 0 0" &amp;gt;&amp;gt; /etc/fstab
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we'll mount our LVM volume and create some mount points for our Gluster volumes-to-be. We'll have separate Gluster volumes for our hosted engine, and for our oVirt data, iso and export domains:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# mount -a
[each-host]# mkdir -p /gluster/{engine,data,iso,export}/brick
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I'd like to use firewalld for my firewall, but at this point oVirt gets along better with good old iptables. Rather than hand-edit my firewall configuration, I start out with no firewall active at all, and I allow oVirt engine to handle firewall configuration itself a bit later in the process:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# systemctl stop firewalld  &amp;amp;&amp;amp; systemctl mask firewalld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now start the Gluster service and configure it to auto-start after subsequent reboots:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# systemctl start glusterd &amp;amp;&amp;amp; systemctl enable glusterd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I mentioned up top that it's no longer necessary to come up with a virtual IP to access Gluster via NFS, but I still want some failover for the mount point that oVirt will use to mount Gluster volumes in the system. I want round robin DNS to associate the storage network IP on each of my three nodes with a common host name that I'll use when I configure my Gluster volumes in oVirt. I could set this up in my network's DNS server, but for a more self-contained solution, I'm using dnsmasq on each of the three nodes.&lt;/p&gt;

&lt;p&gt;I added &lt;code&gt;nameserver 127.0.0.1&lt;/code&gt; to the top of &lt;code&gt;/etc/resolv.conf&lt;/code&gt; on each of my machines, edited &lt;code&gt;/etc/hosts&lt;/code&gt; to make three entries for my mount address, and enabled/started dnsmasq. If you're using a separate network for Gluster, use IPs from that network here:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[each-host]# vi /etc/hosts

$HOST1  glustermount
$HOST2  glustermount
$HOST3  glustermount

[each-host]# yum install dnsmasq -y
[each-host]# systemctl  enable dnsmasq &amp;amp;&amp;amp; systemctl start dnsmasq
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="create-your-gluster-volumes-run-on-host-one"&gt;Create your Gluster volumes (run on host one)&lt;/h3&gt;

&lt;p&gt;This next set of steps need be run on only one of your hosts.&lt;/p&gt;

&lt;p&gt;First, from $HOST1 we'll add $HOST2 and $HOST3 to our gluster trusted pool. Again, If you're using a separate network for Gluster, use IPs from that network here:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[host-one]# gluster peer probe $HOST2
[host-one]# gluster peer probe $HOST3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, we'll create our four gluster volumes. Only the engine volume must be a replica 3 volume to proceed, but if you want your other three volumes to remain accessible in case one of the nodes goes down, you'll need to make them replica 3 as well. Because the gluster arbiter volume type stores data on the first two bricks listed, and only file names and metadata on the third brick, I'm shuffling the order of the bricks in order to spread the load around somewhat:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[host-one]# gluster volume create engine replica 3 arbiter 1 $HOST1:/gluster/engine/brick $HOST2:/gluster/engine/brick $HOST3:/gluster/engine/brick
[host-one]# gluster volume create data replica 3 arbiter 1 $HOST3:/gluster/data/brick $HOST1:/gluster/data/brick $HOST2:/gluster/data/brick
[host-one]# gluster volume create iso replica 3 arbiter 1 $HOST2:/gluster/iso/brick $HOST3:/gluster/iso/brick $HOST1:/gluster/iso/brick
[host-one]# gluster volume create export replica 3 arbiter 1 $HOST1:/gluster/export/brick $HOST2:/gluster/export/brick $HOST3:/gluster/export/brick
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, apply a set of virt-related volume options to our engine and data volumes, and properly set our volume permissions:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[host-one]# gluster volume set engine group virt
[host-one]# gluster volume set data group virt
[host-one]# gluster volume set engine storage.owner-uid 36 &amp;amp;&amp;amp; gluster volume set engine storage.owner-gid 36
[host-one]# gluster volume set data storage.owner-uid 36 &amp;amp;&amp;amp; gluster volume set data storage.owner-gid 36
[host-one]# gluster volume set iso storage.owner-uid 36 &amp;amp;&amp;amp; gluster volume set iso storage.owner-gid 36
[host-one]# gluster volume set export storage.owner-uid 36 &amp;amp;&amp;amp; gluster volume set export storage.owner-gid 36
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we need to start our volumes:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[host-one]# gluster volume start engine
[host-one]# gluster volume start data
[host-one]# gluster volume start iso
[host-one]# gluster volume start export
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="installing-the-hosted-engine-run-on-host-one"&gt;Installing the hosted engine (run on host one)&lt;/h2&gt;

&lt;p&gt;PXE, ISO image, and OVF-formatted disk image are our installation media options for the VM that will host our engine. Here, I'm using the OVF image provided by the &lt;code&gt;ovirt-engine-appliance&lt;/code&gt; package:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[host-one]# yum install ovirt-engine-appliance -y
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we should be ready to fire up &lt;code&gt;screen&lt;/code&gt; (this comes in handy in case of network interruption), kick off the installation process, and begin answering the installer's, answering questions.&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[host-one]# screen
[host-one]# hosted-engine --deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id="storage-configuration"&gt;Storage configuration&lt;/h4&gt;

&lt;p&gt;Here you need to specifiy the &lt;code&gt;glusterfs&lt;/code&gt; storage type, and supply the path to your Gluster volume. If you're using a different sort of shared storage, specify the storage type and location details that apply.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-storage-configuration.png" /&gt;&lt;/p&gt;

&lt;h4 id="network-configuration"&gt;Network configuration&lt;/h4&gt;

&lt;p&gt;Next, we need to specify which network interface to use for oVirt's management network, and whether the installer should configure our firewall. Decline the firewall configuration offer for now – we'll take care of this a bit later.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-network-config.png" /&gt;&lt;/p&gt;

&lt;h4 id="vm-configuration"&gt;VM configuration&lt;/h4&gt;

&lt;p&gt;Now, we answer a set of questions related to the virtual machine that will serve the oVirt engine application. First, we tell the installer to use the oVirt Engine Appliance image that we downloaded earlier:&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-vm-config.png" /&gt;&lt;/p&gt;

&lt;p&gt;Then, we configure cloud-init to customize the appliance on its initial boot, providing host name and network configuration information. We can configure the VM to kick off the engine setup process automatically, but I've found that making this choice has left me without the option to enable Gluster volume management in my oVirt cluster, so we'll run that installer separately in a bit.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-vm-config-a.png" /&gt;&lt;/p&gt;

&lt;p&gt;This part of the installer also prompts you to choose the number of virtual CPUs and RAM for your engine VM. The optimal amount of RAM is 16GB, and the recommended minimum is 4GB.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-hosted-engine-config.png" /&gt;&lt;/p&gt;

&lt;p&gt;Once you've supplied all these answers, and confirmed your choices, the installer will configure the host for virtualization, set up a storage domain, upload the appliance image to that domain, and then launch the engine VM.&lt;/p&gt;

&lt;p&gt;Next, the installer will provide you with an address and password for accessing the VM with the vnc client of your choice. You can fire up a vnc client, enter the address provided and enter the password provided to access the VM, or you can access your engine VM via ssh using the hostname you chose above, which is what I prefer:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[engine-vm]# engine-setup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-engine-setup.png" /&gt;&lt;/p&gt;

&lt;p&gt;Go through the engine-setup script, answering its questions. For the most part, you'll be fine accepting the default answers, but opt out of creating an NFS share for the ISO domain. We'll be creating a Gluster-backed domain for our ISO images in a bit.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-config-preview.png" /&gt;&lt;/p&gt;

&lt;p&gt;When the installation process completes, head back to the terminal where you're running the hosted engine installer and hit enter to indicate that the engine configuration is complete.&lt;/p&gt;

&lt;p&gt;The installer will register itself as a virtualization host on the oVirt engine instance we've just installed.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-connect-engine.png" /&gt;&lt;/p&gt;

&lt;p&gt;Once this completes, the installer will tell you to shut down your VM so that the ovirt-engine-ha services on the first host can restart the engine VM as a monitored service.&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[engine-vm]# poweroff
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id="configuring-hosts-two-and-three"&gt;Configuring Hosts Two and Three&lt;/h3&gt;

&lt;p&gt;Once the engine is back up and available, head to your &lt;strong&gt;second&lt;/strong&gt; machine to configure it as a second host for our oVirt management server:&lt;/p&gt;

&lt;pre class="highlight plaintext"&gt;&lt;code&gt;[host-two]# screen
[host-two]# hosted-engine --deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As with the first machine, the script will ask for the storage type we wish to use. Just as before, answer &lt;code&gt;glusterfs&lt;/code&gt;and then provide the information for your gluster volume, just as on host one.&lt;/p&gt;

&lt;p&gt;After accessing your storage, the script will detect that there's an existing hosted engine instance, and ask whether you're setting up an additional host. Answer yes, and when the script asks for a Host ID, make it &lt;code&gt;2&lt;/code&gt;. The script will then ask for the IP address and root password of your first host, in order to access the rest of the settings it needs.&lt;/p&gt;

&lt;p&gt;When the installation process completes, head over to your &lt;strong&gt;third machine&lt;/strong&gt; and perform the same steps you did w/ your second host, substituting &lt;code&gt;3&lt;/code&gt; for the Host ID.&lt;/p&gt;

&lt;p&gt;Once that process is complete, the script will exit and you should be ready to configure storage and run a VM.&lt;/p&gt;

&lt;h2 id="configuring-storage"&gt;Configuring storage&lt;/h2&gt;

&lt;p&gt;Head to your oVirt engine console at the address of your hosted engine VM, log in with the user name &lt;code&gt;admin&lt;/code&gt; and the password you chose during setup, and visit the "Storage" tab in the console.&lt;/p&gt;

&lt;p&gt;Click "New Domain," give your new domain a name, and choose "Data" and "GlusterFS" from the "Domain Function" and "Storage Type" drop down menus. (If you're using a different sort of external storage, you can choose an option matching that, instead.)&lt;/p&gt;

&lt;p&gt;In the "Path" field, enter the remote path to your Gluster data volume (in my case, this is &lt;code&gt;glustermount:data&lt;/code&gt;), and hit the OK button to proceed.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-data-domain.png" /&gt;&lt;/p&gt;

&lt;p&gt;The export and iso domains, which oVirt uses, respectively, for import and export of VM images, and for storing iso images, can be set up in roughly the same way. Click "New Domain," choose Export or ISO from the "Domain Function" drop down, choose GlusterFS from the "Storage Type" drop down, give the domain a name, and fill in the correct "Path."&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-export-domain.png" /&gt;&lt;/p&gt;

&lt;p&gt;So far, we've created all of our Gluster-backed storage domains as replica 3 arbiter 1 volumes, which ensures that we can bring down one of our nodes at a time while keeping our storage available and consistent. There's a performance cost to replication, however, so I typically create one or more non-replicated Gluster volume-backed oVirt data domains, for times when I favor speed over availability.&lt;/p&gt;

&lt;p&gt;Within an oVirt data center, it's easy to migrate VM storage from one data domain to another, so you can shuttle disks around as needed when it's time to bring one of your storage hosts down.&lt;/p&gt;

&lt;h2 id="enabling-gluster-management"&gt;Enabling Gluster management&lt;/h2&gt;

&lt;p&gt;Head over to the "Clusters" tab in the engine web admin console, right-click the "Default" cluster entry, and choose "Edit" from the context menu. Then, check the box next to "Enable Gluster Service," and hit the "OK" button. This will allow us to manage and view our Gluster volumes from within the web console. It will also prompt the engine to configure our host firewalls correctly the next time the hosts are reinstalled.&lt;/p&gt;

&lt;p&gt;To complete this process, head to the "Hosts" tab, right-click on one of the hosts that isn't currently hosting the engine VM (if you've been following along, the VM should still be on hosted_engine_1), and choose "Maintenance" from the context menu. When the host is in maintenance mode, right-click again and choose "Reinstall" from the menu, and then hit "OK" in the following dialog box. The engine will run through its host installation process, which will include correctly configuring the firewall to allow for the operation both of oVirt and of Gluster.&lt;/p&gt;

&lt;p&gt;When the host is back in "Up" status, repeat on the next engine VM-less host. When that host is reconfigured, head to the "Virtual Machines" tab, right-click on the HostedEngine VM, and choose "Migrate" from the context menu. When the hosted engine migration is complete, carry out the same "Reinstall" operation on the last host. Once this has finished, all three hosts will have their firewalls properly configured.&lt;/p&gt;

&lt;h2 id="running-your-first-vm"&gt;Running your first VM&lt;/h2&gt;

&lt;p&gt;Since version 3.4, oVirt engine has come pre-configured with a public Glance instance managed by the oVirt project. We'll tap this resource to launch our first VM.&lt;/p&gt;

&lt;p&gt;From the storage tab, you should see an "ovirt-image-repository" entry next to a little OpenStack logo. Clicking on this domain will bring up a menu of images available in this repository. Click on the "CirrOS" image (which is very small and perfect for testing) in the list and then click "Import," before hitting the OK button in the pop-up dialog to continue.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-import-image.png" /&gt;&lt;/p&gt;

&lt;p&gt;The image will be copied from the oVirt project's public Glance repository to the storage domain you just configured, where it will be available as a disk to attach to a new VM. In the import image dialog, you have the option of clicking the "Import as Template" check box to give yourself the option of basing multiple future VMs on this image using oVirt's templates functionality.&lt;/p&gt;

&lt;p&gt;Next, head to the "Virtual Machines" tab in the console, click "New VM," choose "Linux" from the "Operating System" drop down menu, supply a name for your VM, and choose the "ovirtmgmt/ovirtmgmt" network in the drop down menu next to "nic1." Then, click the "Attach" button under the "Instance Images" heading and check the radio button next to the CirrOS disk image you just imported before hitting the "OK" button to close the "Attach Virtual Disks" dialog, and hitting "OK" again to exit the "New Virtual Machine" dialog.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-run-vm.png" /&gt;&lt;/p&gt;

&lt;p&gt;For additional configuration, such as setting RAM and CPU values and using cloud-init, there's a "Show Advanced Options" button in the dialog, but you can revisit that later.&lt;/p&gt;

&lt;p&gt;Now, back at the Virtual Machines list, right-click your new VM, and choose "Run" from the menu. After a few moments, the status of your new VM will switch from red to green, and you'll be able to click on the green monitor icon next to “Migrate” to open a console window and access your VM.&lt;/p&gt;

&lt;h2 id="storage-network"&gt;Storage network&lt;/h2&gt;

&lt;p&gt;I mentioned above that it's a good idea to set aside a separate storage network for Gluster traffic and for VM migration. If you've set up a separate network for Gluster traffic, you can bring it under oVirt's management by visiting the "Networks" tab in the web console, clicking "New," and giving your network a name before hitting "OK" to close the dialog.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-storage-net.png" /&gt;&lt;/p&gt;

&lt;p&gt;Next, highlight the new network, and in the &lt;strong&gt;bottom pane&lt;/strong&gt;, choose the "Hosts" tab, and then click the radio button next to "Unattached." One at a time, highlight each of your hosts, click on "Setup Host Networks," and drag the new network you created from the list of "Unassigned Logical Networks" to the interface you're already using for your Gluster traffic, before clicking OK.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-storage-net-a.png" /&gt;&lt;/p&gt;

&lt;p&gt;Then, also in the &lt;strong&gt;bottom pane&lt;/strong&gt;, choose the "Clusters" tab, right-click the "Default" cluster, and choose "Manage Network" from the context menu. Then check the "Migration Network" and "Gluster Network" boxes and hit the "OK" button to close the dialog.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="/images/uarwo36-storage-net-b.png" /&gt;&lt;/p&gt;

&lt;h2 id="maintenance-failover-and-storage"&gt;Maintenance, failover, and storage&lt;/h2&gt;

&lt;p&gt;The key thing to keep in mind regarding host maintainence and downtime is that this converged three node system relies on having at least two of the nodes up at all times. If you bring down two machines at once, you'll run afoul of the Gluster quorum rules that guard us from split-brain states in our storage, the volumes served by your remaining host will go read-only, and the VMs stored on those volumes will pause and require a shutdown and restart in order to run again.&lt;/p&gt;

&lt;p&gt;You can bring a single machine down for maintenance by first putting the system into maintenance mode from the oVirt console, and updating, rebooting, shutting down, etc. as desired.&lt;/p&gt;

&lt;p&gt;Putting a host into maintenance mode will also put that host's hosted engine HA services into local maintenance mode, rendering that host ineligible to take over engine-hosting duties. Previous versions of oVirt enabled administrators to toggle the hosted engine ha maintenance mode from the web admin, but that option appears to have been removed in oVirt 3.6.&lt;/p&gt;

&lt;p&gt;To check on and modify hosted engine ha status, you need to head back to the command line to run &lt;code&gt;hosted-engine --vm-status&lt;/code&gt;. If your host's "Local maintenance" status is "True," you can return it to engine-hosting preparedness with the command &lt;code&gt;hosted-engine --set-maintenance --mode=none&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Also worth noting, if you want to bring down the engine service itself, you can put your whole trio of hosts into global maintenance mode, preventing them from attempting to restart the engine on their own, with the command &lt;code&gt;hosted-engine --set-maintenance --mode=global&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id="till-next-time"&gt;Till next time&lt;/h2&gt;

&lt;p&gt;If you run into trouble following this walkthrough, I’ll be happy to help you get up and running or get pointed in the right direction. On IRC, I’m jbrooks, ping me in the #ovirt room on OFTC or give me a shout on Twitter &lt;a href="https://twitter.com/jasonbrooks"&gt;@jasonbrooks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’re interested in getting involved with the oVirt Project, you can find all the mailing list, issue tracker, source repository, and wiki information you need &lt;a href="http://www.ovirt.org/Community"&gt;here&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>My Devconf.CZ 2016 experience</title>
    <link rel="alternate" href="http://ovirt.org/blog/2016/03/devconf-2016/"/>
    <id>http://ovirt.org/blog/2016/03/devconf-2016/</id>
    <published>2016-03-10T15:00:00+01:00</published>
    <updated>2016-05-18T16:20:26+02:00</updated>
    <author>
      <name>Yaniv Kaul</name>
    </author>
    <content type="html">&lt;p&gt;On the first weekend of February I had the pleasure of attending DevConf.CZ 2016, which took place in the wonderful city of Brno, Czech Republic.
It's a relaxed, young and vibrant conference and it was fun and rewarding from my perspective.
Here's a disorganized personal summary:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Met many community members from all over Europe. Was excited to see so many happy users of the project!&lt;/li&gt;
  &lt;li&gt;Attended &lt;a title="in-depth look of virtual machine migration algorithms - Marcelo Tosatti" href="https://www.youtube.com/watch?v=XkMIMJKJeTY"&gt;in-depth look of virtual machine migration algorithms - Marcelo Tosatti&lt;/a&gt;. I hope to see some of it in coming versions of QEMU, while we work on other improvements to this critical feature.&lt;/li&gt;
  &lt;li&gt;&lt;a title="Growing the ARM server ecosystem - Jim Perrin" href="https://www.youtube.com/watch?v=q4gU87wdRtI"&gt;Growing the ARM server ecosystem - Jim Perrin&lt;/a&gt; was interesting. Seems like a great positive effort, but the road is still long for complete support. Would be cool to support ARM based hosts running virtual machines!&lt;/li&gt;
  &lt;li&gt;Attended &lt;a title="Qemu Disk I/O: Which performs better, Native or Threads? - Pradeep K Surisetty" href="https://www.youtube.com/watch?v=Jx93riUF5_I"&gt;Qemu Disk I/O: Which performs better, Native or Threads? - Pradeep K Surisetty&lt;/a&gt;. The results were not conclusive, though some improvements in native brought it close to, or in some cases better than threads. Seems that our default of heuristic for threads in file-based storage and native in block based storage is fine for the time being, but we'll watch closely for developments in this area.&lt;/li&gt;
  &lt;li&gt;Attended &lt;a title="Debugging the Virtualization Layer (libvirt and QEMU) in OpenStack - Kashyap Chamarthy" href="https://www.youtube.com/watch?v=Dd2AGGMWXQM"&gt;Debugging the Virtualization Layer (libvirt and QEMU) in OpenStack - Kashyap Chamarthy&lt;/a&gt; - I think (and asked) if libvirt can do a better job - in saving in a cyclic log the QMP commands sent and the response from the guest and dump it in case of a guest crash.&lt;/li&gt;
  &lt;li&gt;Paid a visit to &lt;a title="Cockpit Hackfest - Dominik Perpeet, Marius Vollmer, Peter Volpe, Stef Walter" href="https://www.youtube.com/watch?v=TNDe90WSZow"&gt;Cockpit Hackfest - Dominik Perpeet, Marius Vollmer, Peter Volpe, Stef Walter&lt;/a&gt; - as we intend to use Cockpit as our UI for oVirt Next Generation Node, it was great to present our use case and exchange thoughts, ideas and directions. Few bugs were filed during the hackfest per our comments.&lt;/li&gt;
  &lt;li&gt;Attended &lt;a title="Dockerizing JBoss Products - David Becvarik" href="https://www.youtube.com/watch?v=NpyEoFlDzOQ"&gt;Dockerizing JBoss Products - David Becvarik&lt;/a&gt;. Great work done by the JBoss team to improve and streamline packaging of JBoss into containers.&lt;/li&gt;
  &lt;li&gt;Attended &lt;a title="Upstream First Testing - Tim Flink" href="https://www.youtube.com/watch?v=15-yXOJuonQ"&gt;Upstream First Testing - Tim Flink&lt;/a&gt;. I discussed with the presenter the possibility to run oVirt with &lt;a title="Lago project on GitHub" href="https://github.com/lago-project/lago"&gt;Lago project on GitHub&lt;/a&gt; in Fedora, in a CI way, by monitoring distgit changes for relevant packages (such as lvm2, device-mapper*, libvirt and others.&lt;/li&gt;
  &lt;li&gt;Attended &lt;a title="High performance VMs in OpenStack - Nikola Dipanov" href="https://www.youtube.com/watch?v=9J_sEdlpIlQ"&gt;High performance VMs in OpenStack - Nikola Dipanov&lt;/a&gt; - looks like oVirt is already doing a lot of what OpenStack is working on to achieve high-performance from KVM!&lt;/li&gt;
  &lt;li&gt;Preached on running Lago with OpenStack, Gluster and Cockpit. At least the first two items look really promising and I'm looking forward to a collaboration in those efforts. I might just try to do the Gluster one myself.&lt;/li&gt;
  &lt;li&gt;&lt;a title="Avocado and Jenkins: Test Automation and CI - Lukáš Doktor, Yash Mankad" href="https://www.youtube.com/watch?v=rqavfmPAt7o"&gt;Avocado and Jenkins: Test Automation and CI - Lukáš Doktor, Yash Mankad&lt;/a&gt; was interesting for me, as Avocado seems like a cool, mature testing framework - which I hope we can use in the future in CI testing.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Having used the &lt;a href="download/ovirt-live/&amp;quot;download/ovirt-live/&amp;quot;"&gt;oVirt Live&lt;/a&gt; USB DoK image in both FOSDEM and DevConf, I've found several items where we can improve in and filed relevant RFEs for them [1][2].&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The oVirt team has delivered numerous presentations in the virtualization track:
&lt;a title="Smart VM Scheduling - Martin Sivák" href="https://www.youtube.com/watch?v=cQqJEiK7-Ug"&gt;Smart VM Scheduling - Martin Sivák&lt;/a&gt;
&lt;a title="Host fencing in oVirt - Fixing the unknown and allowing VMs to be highly available - Martin Peřina" href="https://www.youtube.com/watch?v=V1JQtmdleaM"&gt;Host fencing in oVirt - Fixing the unknown and allowing VMs to be highly available - Martin Peřina&lt;/a&gt; ,
&lt;a title="Ceph integration with oVirt using Cinder - Nir Soffer" href="https://www.youtube.com/watch?v=4CbHTAkVDZo"&gt;Ceph integration with oVirt using Cinder - Nir Soffer&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The Gluster team has additionally provided more presentations:
&lt;a title="oVirt and Gluster Hyperconvergence - Ramesh Nachimuthu" href="https://www.youtube.com/watch?v=XudYwEWQF7U"&gt;oVirt and Gluster Hyperconvergence - Ramesh Nachimuthu&lt;/a&gt; ,
&lt;a title="Improvements in gluster for virtualization use case - Prasanna Kumar Kalever" href="https://www.youtube.com/watch?v=TczVVCbm8NE"&gt;Improvements in gluster for virtualization use case - Prasanna Kumar Kalever&lt;/a&gt;, which were all well received.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I'd like to thank our project's current community manager Mikey Ariel, and former community manager Brian Proffitt for once again, just one week after FOSDEM, leading the effort of representing the oVirt project and community in this event.&lt;/p&gt;

&lt;p&gt;1.&lt;a title="[RFE] register .vv files so they'll be opened automatically with remote-viewer" href="https://bugzilla.redhat.com/show_bug.cgi?id=1307261"&gt;[RFE] register .vv files so they'll be opened automatically with remote-viewer&lt;/a&gt;
2. &lt;a title="[RFE] Disable power management (display and computer" href="https://bugzilla.redhat.com/show_bug.cgi?id=1307262"&gt;[RFE] Disable power management (display and computer&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Welcome to the new oVirt.org website!</title>
    <link rel="alternate" href="http://ovirt.org/blog/2016/02/welcome-to-new-ovirt-site/"/>
    <id>http://ovirt.org/blog/2016/02/welcome-to-new-ovirt-site/</id>
    <published>2016-02-19T15:00:00+01:00</published>
    <updated>2016-06-02T10:15:55+02:00</updated>
    <author>
      <name>mariel</name>
    </author>
    <content type="html">&lt;p&gt;As part of our efforts to upgrade the ovirt.org website and improve the community experience, we migrated the oVirt website from a MediaWiki site to a static site, authored in &lt;a href="https://help.github.com/articles/basic-writing-and-formatting-syntax/"&gt;Markdown&lt;/a&gt; and published with &lt;a href="https://middlemanapp.com/"&gt;Middleman&lt;/a&gt;. This was a major project that took more than 6 months and involved many contributors from all aspects of the project.&lt;/p&gt;

&lt;p&gt;I'd like to take this opportunity to thank all the people who were involved with this migration, from content reviewers to UX designers and Website admins who gave their time and brain power to make this happen.&lt;/p&gt;

&lt;p&gt;The old MediaWiki site is &lt;a href="http://old.ovirt.org/Home"&gt;still available in read-only&lt;/a&gt;, and will be taken offline once we fix some pending issues, including handling PDF files and such.&lt;/p&gt;

&lt;h2 id="whats-new"&gt;What's new?&lt;/h2&gt;

&lt;p&gt;The new Website is full of improvements and enhancements, check out these highlights:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Source content is now formatted in Markdown instead of MediaWiki. This means that you can create and edit documentation, blog posts, and feature pages with the same Markdown syntax you know.&lt;/li&gt;
  &lt;li&gt;The Website is deployed with Middleman and stored on GitHub. This means that you can make changes to content with the same GitHub contribution workflow that you know (fork, clone, edit, commit, submit pull request). We even have an "Edit this page on GitHub" link at the bottom of every page!&lt;/li&gt;
  &lt;li&gt;New layout and design, from breadcrumbs to sidebards and an upgraded landing page.&lt;/li&gt;
  &lt;li&gt;Automatic redirects from the old MediaWiki site. This means that if the wiki page exists in the new website, previously-released URLs will redirect to that page. If the page was removed, the Search page will open with the page title auto-filled in the search box.&lt;/li&gt;
  &lt;li&gt;Hierarchical content structure. This means that instead of flat Wiki-style files, the deployed Website reflects an organized source repo with content sorted into directories and sub-directories.&lt;/li&gt;
  &lt;li&gt;Official oVirt blog! This first post marks the beginning of our new blog, and we welcome contributions. This means that if you solved a problem with oVirt, want to share your oVirt story, or describe a cool integration, you can submit a blog post and we will provide editorial reviews and help publish your posts.&lt;/li&gt;
  &lt;li&gt;Standardized contribution process. The GitHub repo now includes a &lt;a href="https://github.com/oVirt/ovirt-site/blob/master/README.md"&gt;README.md&lt;/a&gt; file that you can use to learn about how to add and edit content on the website. We welcome pull requests!&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="known-issues"&gt;Known Issues&lt;/h1&gt;

&lt;p&gt;Despite our best efforts, there are still a few kinks with the new website that you should be aware of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Attempting to navigate to ovirt.org (without www.) leads to a redirect loop. We have a ticket open with OpenShift, our hosting service to fix this.&lt;/li&gt;
  &lt;li&gt;Only http is available. We also have a ticket with OpenShift to add SSL and enable https.&lt;/li&gt;
  &lt;li&gt;Home page and Download page are still being upgraded by our UX team, expect some cool new changes soon!&lt;/li&gt;
  &lt;li&gt;Feature pages look-and-feel is still under construction. You can still edit and push feature pages as usual.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id="whats-next"&gt;What's Next&lt;/h1&gt;

&lt;p&gt;Even though the Website is live, the work is hardly over. We'd like to ask for your help in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reviewing content for anything obsolete or outdated; each page in the new website includes a header toolbar with metadata from the original wiki page for your convenience&lt;/li&gt;
  &lt;li&gt;Submitting blog posts or any other content that you wish to share with the oVirt community&lt;/li&gt;
  &lt;li&gt;Reporting bugs and proposing enhancements, for example broken links or missing pages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We hope you will enjoy the new oVirt Website, looking forward to your feedback and contributions!&lt;/p&gt;
</content>
  </entry>
</feed>
